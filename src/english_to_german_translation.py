# -*- coding: utf-8 -*-
"""english_to_german_translation.ipynb

Automatically generated by Colaboratory.


#### <font color='crimson'> Mount to Google Drive </font>
"""

from google.colab import drive
drive.mount('/content/drive')

"""#### <font color='crimson'> Import Python Libraries </font>"""

import numpy as np
import pandas as pd
import pandas as pd
import regex as re
import spacy
from sklearn.feature_extraction.text import CountVectorizer

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

import warnings
warnings.simplefilter('ignore')

from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, RepeatVector
from keras.preprocessing.text import Tokenizer
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.sequence import pad_sequences
from keras.models import load_model
from keras import optimizers

from tqdm.notebook import tqdm
tqdm.pandas()

"""#### <font color='crimson'> Step 1: Import and merge all the three datasets </font>"""

def read_file(filepath):
    with open(filepath,"r",encoding='utf-8') as f:
        file = f.readlines()
    return file


def read_all_files(filepaths):
    complete_df = {}
    for f in filepaths:
        de = read_file('drive/MyDrive/colab_notebook/great-learning-projects/nlp-language-translation/data/'+f[0])
        en = read_file('drive/MyDrive/colab_notebook/great-learning-projects/nlp-language-translation/data/'+f[1])

        df = pd.DataFrame([de, en]).T
        df.columns = ['german', 'english']

        if len(complete_df)==0:
            complete_df = df.copy()
        else:
            complete_df = pd.concat([complete_df, df])
    return complete_df


filepaths = [
                ['news-commentary-v9.de-en.de', 'news-commentary-v9.de-en.en'],
                #['europarl-v7.de-en.de','europarl-v7.de-en.en'],
                #['commoncrawl.de-en.de','commoncrawl.de-en.en']
            ]

df = read_all_files(filepaths)
df = df.head(10000) # For faster model training, Use complete for final report
print(df.shape)
df.head()

"""#### <font color='crimson'> Step 2: Data cleansing </font>
- Drop Null Values
- Remove Leading and Trailing spaces
- Convert to lower case
- Keep only alphabets by removing special characters and numbers
- Remove English and German stopwords
- Drop Duplicate Records
"""

df.dropna(inplace=True)
print(df.shape)

df['german']  = df['german'].progress_apply(lambda x: str(x).strip())
df['english'] = df['english'].progress_apply(lambda x: str(x).strip())

for c in tqdm(df.columns):
    df[c] = df[c].progress_apply(lambda x: str(x).lower())

for c in tqdm(df.columns):
    df[c] = df[c].progress_apply(lambda x: re.sub(r'[^a-zA-Z ]+', '', str(x)))

df.head()

#german_stop_words = stopwords.words('german')
#english_stop_words = stopwords.words('english')

#df['german'] = df['german'].progress_apply(lambda x: ' '.join([i for i in str(x).split(' ') if i not in german_stop_words]))
#df['english'] = df['english'].progress_apply(lambda x: ' '.join([i for i in str(x).split(' ') if i not in english_stop_words]))

df.drop_duplicates(inplace=True)
print(df.shape)
display(df.head())

"""#### <font color='crimson'> Step 3: NLP pre processing - Dataset suitable to be used for AIML model learning </font>

- Tokenize English &  German
"""

def get_tokenization(data):

  def tokenization(data):
    t = Tokenizer()
    t.fit_on_texts(data)
    return t

  t = tokenization(data)
  vocab_size = len(t.word_index) + 1
  length = 8
  return t, vocab_size, length

english_tokenizer, english_vocab_size, english_length = get_tokenization(df['english'])
german_tokenizer, german_vocab_size, german_length    = get_tokenization(df['german'])

print('English vocab size and length:', english_vocab_size, english_length)
print('German  vocab size and length:', german_vocab_size, german_length)

"""- Sequence and padding - for equal length"""

def encode_to_sequence(tokenizer, data, length):
  seq = tokenizer.texts_to_sequences(data)
  seq = pad_sequences(seq, maxlen=length, padding='post')
  return seq

english_sequence = encode_to_sequence(english_tokenizer, df['english'], 8)
german_sequence  = encode_to_sequence(german_tokenizer, df['german'], 8)

english_sequence.shape, german_sequence.shape

"""#### <font color='crimson'> Step 4: Design, train and test simple RNN & LSTM model </font>

- Train Test Split - for model evaluation
"""

train_data_size = int(0.8*len(df))

train_english_data_as_x, train_german_data_as_y = english_sequence[:train_data_size], german_sequence[:train_data_size]
test_english_data_as_x, test_german_data_as_y = english_sequence[train_data_size:], german_sequence[train_data_size:]

print('Train Data =', len(train_english_data_as_x), len(train_german_data_as_y))
print('Test  Data =', len(test_english_data_as_x), len(test_german_data_as_y))

"""- LSTM Model Definition"""

def build_model(input_vocab_size, output_vocab_size, input_length, output_length, units):

  model = Sequential()
  model.add(Embedding(input_vocab_size, units, input_length=input_length, mask_zero=True))
  model.add(LSTM(units))
  model.add(RepeatVector(output_length))
  model.add(LSTM(units, return_sequences=True))
  model.add(Dense(output_vocab_size, activation='softmax'))
  return model

model = build_model(english_vocab_size, german_vocab_size, german_length, english_length, 512)

"""- Setting up Optimizer"""

optimizer_setting = optimizers.RMSprop(learning_rate=0.001)

"""- Model Compilation"""

model.compile(optimizer=optimizer_setting, loss='sparse_categorical_crossentropy')

"""- Model Training"""

g_shape = train_german_data_as_y.shape
reshaped_train_german_data_as_y = train_german_data_as_y.reshape(g_shape[0], g_shape[1],1)

# 2 epochs are used to get results faster, epoch = 10 will be used in final for better results
actual_fits = model.fit(train_english_data_as_x, reshaped_train_german_data_as_y, epochs=2, validation_split=0.2)

"""**Please note**:

Loss is higher as data volume used for model training is significantly low i.e. 10K rows from 1 dataset only with number of epochs reduced to 2 to obtain results faster.

For actual and final execution, all datasets and rows will be considered with epochs=10 and using batch size. That should reduce the loss from 7.6 significantly lower.

- Model Prediction
"""

test_english_data_as_x[:2]

e_shape = test_english_data_as_x.shape
reshaped_test_english_data_as_x = test_english_data_as_x.reshape(e_shape[0], e_shape[1],1)

prediction = model.predict(reshaped_test_english_data_as_x)
classes_x  = np.argmax(prediction,axis=1)

"""- Get Model Prediction (Numbers to Words) for comparison

**Please note:**

Since dataset volumn is significantly low and epochs=2, the language translation model will be inferior. Thus, actual vs prediction is not shown below. Reason for choosing smaller dataset is longer model training time, 15-20 Hrs and kernal failure on local machine multiple times.

Also, after using complete data volume from all 3 datasets (or atleast 1 dataset) and sufficient epochs, the actual vs prediction comparison will be meaningful.
"""

def convert_numbers_to_words(tokenizer, i):
  for word, index in tokenizer.word_index.items():
    if index==i:
      return word
  return ''

prediction_df = []
for integers in tqdm(classes_x[:10]): # Testing only for first 10 entries
  german_prediction = [convert_numbers_to_words(german_tokenizer, j) for j in integers]
  prediction_df.append(' '.join(german_prediction))

"""#### <font color='crisom'> Step 5: Interim report </font>"""